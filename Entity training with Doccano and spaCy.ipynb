{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity training with Doccano and spaCy\n",
    "18 September 2020\n",
    "\n",
    "- Train custom NER in spaCy\n",
    "- Finetune entities in Doccano\n",
    "- Re-train NER model in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "from spacy import displacy\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import display, HTML\n",
    "import random\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from doccano_transformer.datasets import NERDataset\n",
    "from doccano_transformer.utils import read_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    2.3.2                         \n",
      "Location         /home/kriesbeck/miniconda3/envs/nlp/lib/python3.7/site-packages/spacy\n",
      "Platform         Linux-4.19.104-microsoft-standard-x86_64-with-debian-buster-sid\n",
      "Python version   3.7.7                         \n",
      "Models                                         \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spaCy version': '2.3.2',\n",
       " 'Location': '/home/kriesbeck/miniconda3/envs/nlp/lib/python3.7/site-packages/spacy',\n",
       " 'Platform': 'Linux-4.19.104-microsoft-standard-x86_64-with-debian-buster-sid',\n",
       " 'Python version': '3.7.7',\n",
       " 'Models': ''}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(ner_model, examples):\n",
    "    \"\"\" Evaluates ner_model against goldparse examples\n",
    "\n",
    "    Args:\n",
    "        ner_model: spaCy NER model to evaluate\n",
    "        examples: goldparse text and entity annotations\n",
    "\n",
    "    Returns:\n",
    "        Scores, including precision, recall, and f1\n",
    "\n",
    "    \"\"\"\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot['entities'])\n",
    "        pred_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_iob_to_spacy(file_path):\n",
    "    \"\"\" Converts data from tab-delimited IOB:\n",
    "    label \\t word \\n label \\t word \\n \\n label \\t word\n",
    "    to spaCy format: \n",
    "    sentence, {entities : [(start, end, label), (start, end, label)]}\n",
    "\n",
    "    Args:\n",
    "        file-path: to IOB data\n",
    "\n",
    "    Returns:\n",
    "        Data in spaCy format\n",
    "        Unique data labels\n",
    "    \"\"\"\n",
    "    file = open(file_path, 'r')\n",
    "    training_data, entities, sentence, unique_labels = [], [], [], []\n",
    "    current_annotation = None\n",
    "    end = 0 # initialize counter to keep track of start and end characters\n",
    "    for line in file:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        # lines with len > 1 are words\n",
    "        if len(line) > 1:\n",
    "            label = line[0][2:] # the .txt is formatted: label \\t word, label[0:2] = label_type\n",
    "            label_type = line[0][0] # beginning of annotations - \"B\", intermediate - \"I\"\n",
    "            word = line[1]\n",
    "            sentence.append(word)\n",
    "            end += (len(word) + 1) # length of the word + trailing space\n",
    "\n",
    "            if label_type != 'I' and current_annotation: # if at the end of an annotation\n",
    "                entities.append((start, end - 2 - len(word), current_annotation)) # append the annotation\n",
    "                current_annotation = None # reset the annotation\n",
    "            if label_type == 'B': # if beginning new annotation\n",
    "                start = end - len(word) - 1 # start annotation at beginning of word\n",
    "                current_annotation = label # append the word to the current annotation\n",
    "            if label_type == 'I': # if the annotation is multi-word\n",
    "                current_annotation = label # append the word\n",
    "\n",
    "            if label != 'O' and label not in unique_labels:\n",
    "                unique_labels.append(label)\n",
    "\n",
    "        # lines with len == 1 are breaks between sentences\n",
    "        if len(line) == 1:\n",
    "            if current_annotation:\n",
    "                entities.append((start, end - 1, current_annotation))\n",
    "            sentence = \" \".join(sentence)\n",
    "            training_data.append([sentence, {'entities' : entities}])\n",
    "            # reset the counters and temporary lists\n",
    "            end = 0\n",
    "            entities, sentence = [], []\n",
    "            current_annotation = None\n",
    "    file.close()\n",
    "    return training_data, unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spacy_to_doccano(data=[], model=None):\n",
    "    \"\"\" Runs NER model on data and converts spaCy formatted output to Doccano JSON format\n",
    "\n",
    "    Args:\n",
    "        data\n",
    "        model: spaCy NER model\n",
    "\n",
    "    Returns:\n",
    "        Entity data in Doccano JSON format\n",
    "\n",
    "    \"\"\"\n",
    "    FINETUNE_JSON = []\n",
    "    for text, _ in data:\n",
    "        doc = model(text)\n",
    "        doc_json = doc.to_json()\n",
    "        labels = []\n",
    "        for ent in doc_json['ents']:\n",
    "            label = []\n",
    "            label.append(ent['start'])\n",
    "            label.append(ent['end'])\n",
    "            label.append(ent['label'])\n",
    "            labels.append(label)\n",
    "        doc_json['labels'] = labels \n",
    "        del doc_json['ents']\n",
    "        del doc_json['tokens']\n",
    "        FINETUNE_JSON.append(doc_json)\n",
    "    return FINETUNE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner_model(model=None, train_data=[], n_iter=10):\n",
    "    \"\"\" Trains spaCy NER model\n",
    "\n",
    "    Args:\n",
    "        train_data: data\n",
    "        model: 'en_core_web_sm' or model name to update a pretrained model or None to train a new model\n",
    "        n_iter: number of iterations\n",
    "\n",
    "    Returns:\n",
    "        Trained spaCy NER model\n",
    "    \"\"\"\n",
    "    # Load pretrained spaCy model or create a blank model\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "\n",
    "    # Get ner pipeline component (create if necessary)\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # Add new entity labels to entity recognizer\n",
    "    for i in LABELS:\n",
    "        ner.add_label(i)\n",
    "\n",
    "    # Set optimizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "\n",
    "    move_names = list(ner.move_names)\n",
    "\n",
    "    # Get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "    # Only train NER pipe\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        # Process our training examples in iterations using shuffle, batches, and dropouts\n",
    "        sizes = compounding(1, 16, 1.001)\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                # For each example, nlp.update steps through the words of the input \n",
    "                # At each word, it makes a prediction on the text and checks the annotations \n",
    "                # If it was wrong, it adjusts its weights\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training data\n",
    "\n",
    "For this example, we'll use the **MIT Movies** corpus, which contains 10,000 queries about various aspects of movies, with the following entity labels: ACTOR, TITLE, GENRE, DIRECTOR, etc.\n",
    "\n",
    "\n",
    "The train and test datasets are available here: https://groups.csail.mit.edu/sls/downloads/movie/\n",
    "\n",
    "They're saved in the data/ directory of this repository, but here are the curl commands for reference:\n",
    "* curl https://groups.csail.mit.edu/sls/downloads/movie/engtest.bio -o data/test.txt\n",
    "* curl https://groups.csail.mit.edu/sls/downloads/movie/engtrain.bio -o data/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other use cases, you could create token or pattern entity matches with spaCy as a starting place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\twhat\n",
      "O\tmovies\n",
      "O\tstar\n",
      "B-ACTOR\tbruce\n",
      "I-ACTOR\twillis\n",
      "\n",
      "O\tshow\n",
      "O\tme\n",
      "O\tfilms\n",
      "O\twith\n",
      "B-ACTOR\tdrew\n",
      "I-ACTOR\tbarrymore\n",
      "O\tfrom\n",
      "O\tthe\n",
      "B-YEAR\t1980s\n",
      "\n",
      "O\twhat\n",
      "O\tmovies\n",
      "O\tstarred\n",
      "O\tboth\n",
      "B-ACTOR\tal\n",
      "I-ACTOR\tpacino\n",
      "O\tand\n",
      "B-ACTOR\trobert\n",
      "I-ACTOR\tdeniro\n",
      "\n",
      "O\tfind\n",
      "O\tme\n",
      "O\tall\n",
      "O\tof\n",
      "O\tthe\n",
      "O\tmovies\n",
      "O\tthat\n",
      "O\tstarred\n",
      "B-ACTOR\tharold\n",
      "I-ACTOR\tramis\n",
      "O\tand\n",
      "B-ACTOR\tbill\n",
      "I-ACTOR\tmurray\n",
      "\n",
      "O\tfind\n",
      "O\tme\n",
      "O\ta\n",
      "O\tmovie\n",
      "O\twith\n",
      "O\ta\n",
      "O\tquote\n",
      "O\tabout\n",
      "O\tbaseball\n",
      "O\tin\n",
      "O\tit\n",
      "\n",
      "O\twhat\n",
      "O\tmovies\n",
      "O\thave\n",
      "B-TITLE\tmississippi\n",
      "O\tin\n",
      "O\tthe\n",
      "O\ttitle\n",
      "\n",
      "O\tshow\n",
      "O\tme\n",
      "B-GENRE\tscience\n",
      "I-GENRE\tfiction\n",
      "I-GENRE\tfilms\n",
      "O\tdirected\n",
      "O\tby\n",
      "B-DIRECTOR\tsteven\n",
      "I-DIRECTOR\tspielberg\n",
      "\n",
      "O\tdo\n",
      "O\tyou\n",
      "O\thave\n",
      "O\tany\n",
      "B-GENRE\tthrillers\n",
      "O\tdirected\n",
      "O\tby\n",
      "B-DIRECTOR\tsofia\n",
      "I-DIRECTOR\tcoppola\n",
      "\n",
      "O\twhat\n",
      "B-SONG\tleonard\n",
      "I-SONG\tcohen\n",
      "I-SONG\tsongs\n",
      "O\thave\n",
      "O\tbeen\n",
      "O\tused\n",
      "O\tin\n",
      "O\ta\n",
      "O\tmovie\n",
      "\n",
      "O\tshow\n",
      "O\tme\n",
      "O\tfilms\n",
      "B-ACTOR\telvis\n",
      "O\tfilms\n",
      "B-PLOT\tset\n",
      "I-PLOT\tin\n",
      "I-PLOT\thawaii\n",
      "\n",
      "O\twhat\n",
      "O\tmovie\n",
      "O\tis\n",
      "O\treferences\n",
      "B-PLOT\tzydrate\n",
      "\n",
      "O\tare\n",
      "O\tthere\n",
      "O\tany\n",
      "B-GENRE\tmusical\n",
      "I-GENRE\tfilms\n",
      "O\twith\n",
      "B-ACTOR\tpatrick\n",
      "I-ACTOR\tdempsey\n"
     ]
    }
   ],
   "source": [
    "# Look at the train data:\n",
    "f = open('data/train.txt', 'r')\n",
    "train = f.read()\n",
    "f.close()\n",
    "\n",
    "print(train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data for spacy\n",
    "TRAIN_DATA, LABELS = convert_iob_to_spacy(\"data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what movies star bruce willis', {'entities': [(17, 29, 'ACTOR')]}],\n",
       " ['show me films with drew barrymore from the 1980s',\n",
       "  {'entities': [(19, 33, 'ACTOR'), (43, 48, 'YEAR')]}],\n",
       " ['what movies starred both al pacino and robert deniro',\n",
       "  {'entities': [(25, 34, 'ACTOR'), (39, 52, 'ACTOR')]}],\n",
       " ['find me all of the movies that starred harold ramis and bill murray',\n",
       "  {'entities': [(39, 51, 'ACTOR'), (56, 67, 'ACTOR')]}],\n",
       " ['find me a movie with a quote about baseball in it', {'entities': []}],\n",
       " ['what movies have mississippi in the title',\n",
       "  {'entities': [(17, 28, 'TITLE')]}],\n",
       " ['show me science fiction films directed by steven spielberg',\n",
       "  {'entities': [(8, 29, 'GENRE'), (42, 58, 'DIRECTOR')]}],\n",
       " ['do you have any thrillers directed by sofia coppola',\n",
       "  {'entities': [(16, 25, 'GENRE'), (38, 51, 'DIRECTOR')]}],\n",
       " ['what leonard cohen songs have been used in a movie',\n",
       "  {'entities': [(5, 24, 'SONG')]}],\n",
       " ['show me films elvis films set in hawaii',\n",
       "  {'entities': [(14, 19, 'ACTOR'), (26, 39, 'PLOT')]}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split off finetune data\n",
    "Currently, the dataset is 80% train and 20% test.  Split off another 20% (25% of train) to use to finetune in Doccano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA, FINETUNE_DATA = train_test_split(TRAIN_DATA, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model 0 in spaCy\n",
    "Initial model training based on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kriesbeck/miniconda3/envs/nlp/lib/python3.7/site-packages/spacy/language.py:639: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  **kwargs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 14856.23525674752}\n",
      "Losses {'ner': 9051.65772494761}\n",
      "Losses {'ner': 7421.706851945608}\n",
      "Losses {'ner': 6602.3245470604015}\n",
      "Losses {'ner': 6156.108316206479}\n",
      "Losses {'ner': 5671.798042498967}\n",
      "Losses {'ner': 5403.023727640749}\n",
      "Losses {'ner': 5053.1297999462695}\n",
      "Losses {'ner': 4851.588826170277}\n",
      "Losses {'ner': 4437.469602505349}\n"
     ]
    }
   ],
   "source": [
    "nlp = train_ner_model(model=None, train_data=TRAIN_DATA, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to output directory\n",
    "nlp.meta[\"name\"] = \"movies_0\"\n",
    "nlp.to_disk('models/movies_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model 0 performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the unseen test data\n",
    "TEST_DATA, _ = convert_iob_to_spacy(\"data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model 0\n",
    "movies_ner = spacy.load('models/movies_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each entity in the test data, we can calculate the precision, recall, f-score, and more:\n",
    "* Precision: true positives / (true positives + false positives)  \n",
    "* Recall: true positives / (true positives + false negatives)  \n",
    "* F1-score: a measure of accuracy; the harmonic average of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model_0 = evaluate_model(movies_ner, TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 84.40779610194903 \n",
      "Recall: 84.36036710994568 \n",
      "F1-score: 84.38407494145198\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {} \\nRecall: {} \\nF1-score: {}\".format(scores_model_0['ents_p'],\n",
    "                                                         scores_model_0['ents_r'],\n",
    "                                                         scores_model_0['ents_f']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model 0 on finetune data\n",
    "Important: Run tuning data through your original model to prevent catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_JSON = convert_spacy_to_doccano(data=FINETUNE_DATA, model=movies_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export JSONL file\n",
    "with open('finetune.txt', 'w') as json_file:\n",
    "    for line in FINETUNE_JSON:\n",
    "        json_record = json.dumps(line)\n",
    "        json_file.write(json_record + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Doccano locally\n",
    "\n",
    "- Follow instructions at https://github.com/doccano\n",
    "- Import finetune.txt\n",
    "- Manually correct annotations\n",
    "- Export Doccano annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Doccano annotations to spaCy format\n",
    "- TODO: Use doccano-transformer to convert Doccano's JSONL export to spaCy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetune_doccano = read_jsonl(filepath='file.json1', dataset=NERDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetune_spacy = finetune_doccano.to_spacy(tokenizer=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file.json1','r') as doccano:\n",
    "    finetune_doccano = []\n",
    "    for line in doccano.readlines():\n",
    "        finetune_doccano.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 23,\n",
       "  'text': 'who starred in the man without a face',\n",
       "  'meta': {},\n",
       "  'annotation_approver': None,\n",
       "  'labels': []},\n",
       " {'id': 24,\n",
       "  'text': 'tell me more about the movie elektra',\n",
       "  'meta': {},\n",
       "  'annotation_approver': None,\n",
       "  'labels': [[29, 36, 'TITLE']]},\n",
       " {'id': 25,\n",
       "  'text': 'find a move by director john cleese',\n",
       "  'meta': {},\n",
       "  'annotation_approver': None,\n",
       "  'labels': []},\n",
       " {'id': 26,\n",
       "  'text': 'what year was ghandi filmed',\n",
       "  'meta': {},\n",
       "  'annotation_approver': None,\n",
       "  'labels': []},\n",
       " {'id': 27,\n",
       "  'text': 'i want to get a must see gangster movie from the past three years',\n",
       "  'meta': {},\n",
       "  'annotation_approver': None,\n",
       "  'labels': []}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_doccano[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to spacy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['could you please recommend a good horror film that was made in 2011',\n",
       "  {'entities': [(34, 40, 'GENRE')]}],\n",
       " ['name the documentary by sergio leone',\n",
       "  {'entities': [(9, 20, 'GENRE'), (24, 36, 'DIRECTOR')]}],\n",
       " ['what are some sport films that was released within the last seven years',\n",
       "  {'entities': [(14, 19, 'GENRE'), (55, 71, 'YEAR')]}],\n",
       " ['do you like that really popular zombie move made by director laura gabbert',\n",
       "  {'entities': [(17, 31, 'RATINGS_AVERAGE'),\n",
       "    (32, 38, 'PLOT'),\n",
       "    (61, 74, 'DIRECTOR')]}],\n",
       " ['show me that man from rio', {'entities': [(8, 25, 'TITLE')]}]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINETUNE_DATA[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model 1 in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3648.0199915609164}\n",
      "Losses {'ner': 2646.55164874975}\n",
      "Losses {'ner': 2022.2504103559575}\n",
      "Losses {'ner': 1687.2290640563742}\n",
      "Losses {'ner': 1374.053428110652}\n",
      "Losses {'ner': 1199.4915336527429}\n",
      "Losses {'ner': 1005.58177761978}\n",
      "Losses {'ner': 916.226192263246}\n",
      "Losses {'ner': 845.9147403139978}\n",
      "Losses {'ner': 780.6000975713796}\n"
     ]
    }
   ],
   "source": [
    "nlp = train_ner_model(model='models/movies_0', train_data=FINETUNE_DATA, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to output directory\n",
    "nlp.meta[\"name\"] = \"movies_1\"\n",
    "nlp.to_disk('models/movies_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model 1\n",
    "movies_ner = spacy.load('models/movies_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model_1 = evaluate_model(movies_ner, TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 83.01462317210348 \n",
      "Recall: 82.9368795654617 \n",
      "F1-score: 82.97573315843717\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {} \\nRecall: {} \\nF1-score: {}\".format(scores_model_1['ents_p'],\n",
    "                                                         scores_model_1['ents_r'],\n",
    "                                                         scores_model_1['ents_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 84.40779610194903 \n",
      "Recall: 84.36036710994568 \n",
      "F1-score: 84.38407494145198\n"
     ]
    }
   ],
   "source": [
    "# compare to Model 0\n",
    "print(\"Precision: {} \\nRecall: {} \\nF1-score: {}\".format(scores_model_0['ents_p'],\n",
    "                                                         scores_model_0['ents_r'],\n",
    "                                                         scores_model_0['ents_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
